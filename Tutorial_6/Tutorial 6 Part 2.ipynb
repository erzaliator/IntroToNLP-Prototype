{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordVec\n",
    "\n",
    "In this tutorial we will be using gensim to study word vectors. \n",
    "\n",
    "\n",
    "Letâ€™s download a pre-trained model and play around with it. We will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases. \n",
    "\n",
    "Such a model can take hours to train, we will load Gensim's pretrained model.\n",
    "\n",
    "<font color='red'> Run the cells in the sections below to understand how to use the gensim model </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Word Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Let us look at the vector representation of the word 'king'. \n",
    "vec_king = wv['king']\n",
    "print(vec_king.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to create a small neural network that has a bottleneck representing each word as a 300d vector, and then train the network to predict nearby \"context\" words given some central word. \n",
    "\n",
    "Over time, it learns the representations that allow it to make the best predictions, essentially capturing the typical context of each word as its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise similarity between two words\n",
    "\n",
    "Word2Vec implements word pair similarity using the cosine distance between the two word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us look at the vector representation of the word 'king' and 'queen'. We find modifying the vector representation of king wrt. man and woman yields a vector representation similar to queen.\n",
    "\n",
    "wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use it to understand semantically similar categories\n",
    "\n",
    "wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 300)\n",
      "[[ 0.109375    0.140625   -0.03173828 ...  0.00765991  0.12011719\n",
      "  -0.1796875 ]\n",
      " [ 0.00704956 -0.07324219  0.171875   ...  0.01123047  0.1640625\n",
      "   0.10693359]\n",
      " [ 0.12597656  0.19042969  0.06982422 ...  0.0612793   0.17285156\n",
      "  -0.07861328]\n",
      " [ 0.20800781 -0.03564453 -0.14941406 ...  0.0534668  -0.20019531\n",
      "   0.03515625]\n",
      " [ 0.11767578 -0.234375    0.4765625  ... -0.24511719  0.1484375\n",
      "   0.07714844]]\n"
     ]
    }
   ],
   "source": [
    "# We can use it to compute representations of sentences. A sentence representation is computed by averaging the word vectors of the words in the sentence.\n",
    "\n",
    "sentence = ['this', 'is', 'an', 'odd', 'sentence']\n",
    "vec_sentence = wv[sentence]\n",
    "print(vec_sentence.shape)\n",
    "print(vec_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_ling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
