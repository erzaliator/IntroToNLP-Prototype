{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing using regex\n",
    "The previous lesson demonstrated using regex to answer Eliza questions. In this lesson, we will use regex to process text. \n",
    "\n",
    "<font color=\"red\"> **Tokenisation** </font> is defined as the process of breaking up a string into tokens. \n",
    "Tokens are the basic building blocks of a sentence. In English, tokens are words, punctuation, and numbers.\n",
    "\n",
    "For German tokenisation, we need to consider the following:\n",
    "- German has compound words, e.g. \"Krankenhaus\" (hospital) is made up of \"Kranken\" (sick) and \"Haus\" (house).\n",
    "- German has umlauts, e.g. \"ä\", \"ö\", \"ü\", \"ß\".\n",
    "- German has a different punctuation system, e.g. \"„\", \"“\", \"–\", \"…\", \"»\", \"«\".\n",
    "\n",
    "Let us focus first on English tokenization. That in itself is not trivial. \n",
    "Web based text can be:\n",
    "1. formal (e.g. news articles when we want to build a news summarizer) \n",
    "2. informal (e.g. twitter when we want to build a sentiment analyzer) Interesting things happen when we deal with modern vocabulary found in informal texts like twitter.\n",
    "\n",
    "Especially when we work with web based texts, we need to consider the following (with examples):\n",
    "- HTML tags (e.g. \\<br\\>)\n",
    "- URLs (http://www.example.com)\n",
    "- Emoticons (:-))\n",
    "- Abbreviations (e.g. \"Mr.\", \"Mrs.\", \"Dr.\", \"Prof.\")\n",
    "- Punctuation (e.g. \"!\", \"?\", \".\", \",\", \";\", \":\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\")\n",
    "- Numbers (e.g. \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\")\n",
    "- Contractions (didn't = did not, capp'n=capping, )\n",
    "- Currency symbols (e.g. \"$\", \"€\", \"£\", \"¥\")\n",
    "- Hyphenated words (e.g. \"self-driving\")\n",
    "- Words with apostrophes (e.g. \"it's\")\n",
    "- Words with underscores (e.g. \"@this_is_a_twitter_handle\")\n",
    "- Special characters (e.g. \"@\", \"#\", \"%\", \"&\", \"*\", \"+\", \"=\", \"~\", \"_\")\n",
    "- Non-ASCII characters (e.g. \"é\", \"ñ\", \"ç\", \"ß\")\n",
    "- Words with periods (e.g. \"U.S.\", \"U.N.\", \"U.d.S.\", \"L.S.T.\")\n",
    "and much more... \n",
    "\n",
    "Today we will cover a subset of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using regex\n",
    "\n",
    "Consider that we are interested in knowing the words spoken by Eliza. \n",
    "Or we want to know the words spoken by the user in a youtube comment. We can use regex to extract the words from the text.\n",
    "\n",
    "sentence = \"Today is 2023-07-20 and we are learning #NLP in class. Check out the website: www.example.com for more information! The price is $100.50.\"\n",
    "\n",
    "Exercise 1: Word Tokenization in NLP\n",
    "\n",
    "In this exercise, we will focus on word tokenization using space-based tokenization and handle special cases such as punctuation, dates, URLs, and hashtags.\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "Step 1: Simple <font color=\"blue\">**Space-Based Tokenization**</font>\n",
    "Write a Python function called simple_tokenizer that takes a sentence as input and returns a list of words, tokenized based on spaces.\n",
    "\n",
    "\n",
    "```\n",
    "def simple_tokenizer(sentence):\n",
    "    # TODO: Implement space-based tokenization\n",
    "    return tokens\n",
    "```\n",
    "\n",
    "Step 2: Handling <font color=\"blue\">**Punctuation**</font>\n",
    "Update the simple_tokenizer function to handle punctuation marks as well. Punctuation marks should be treated as separate tokens.\n",
    "\n",
    "Step 3: Handling <font color=\"blue\">**Dates**</font>\n",
    "Write a Python function called handle_dates that takes a list of tokens and identifies dates in the format \"YYYY-MM-DD\" and tokenizes them as a single unit.\n",
    "\n",
    "```\n",
    "def handle_dates(tokens):\n",
    "    # TODO: Identify dates (YYYY-MM-DD) and merge them into a single token\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "Step 4: Handling <font color=\"blue\">**URLs**</font>\n",
    "Write a Python function called handle_urls that takes a list of tokens and identifies URLs and tokenizes them as a single unit.\n",
    "\n",
    "```\n",
    "def handle_urls(tokens):\n",
    "    # TODO: Identify URLs and merge them into a single token\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "Step 5: Handling <font color=\"blue\">**Hashtags**</font>\n",
    "Write a Python function called handle_hashtags that takes a list of tokens and identifies hashtags and tokenizes them as a single unit.\n",
    "\n",
    "```def handle_hashtags(tokens):\n",
    "    # TODO: Identify hashtags and merge them into a single token\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "Step 6: Handling <font color=\"blue\">**Currency Values**</font>\n",
    "Write a Python function called handle_currency that takes a list of tokens and identifies currency values (e.g., $100.50) and tokenizes them as a single unit.\n",
    "                                                                                                           \n",
    "```\n",
    "    def handle_currency(tokens):\n",
    "    # TODO: Identify currency values and merge them into a single token\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "\n",
    "Step 7: Putting It All Together\n",
    "Now, combine all the functions together to create a comprehensive tokenizer that handles spaces, punctuation, dates, URLs, and hashtags.\n",
    "\n",
    "```\n",
    "def comprehensive_tokenizer(sentence):\n",
    "    tokens = simple_tokenizer(sentence)\n",
    "    tokens = handle_dates(tokens)\n",
    "    tokens = handle_urls(tokens)\n",
    "    tokens = handle_hashtags(tokens)\n",
    "    tokens = handle_currency(tokens)\n",
    "    return tokens\n",
    "```\n",
    "\n",
    "Step 8: Test the Tokenizer\n",
    "Test your comprehensive tokenizer using various sentences containing dates, URLs, hashtags, and punctuation marks.\n",
    "\n",
    "Example:\n",
    "```\n",
    "sentence = \"Today is 2023-07-20 and we are learning #NLP in class. Check out the website: www.example.com for more information!\"\n",
    "tokens = comprehensive_tokenizer(sentence)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "['Today', 'is', '2023-07-20', 'and', 'we', 'are', 'learning', '#NLP', 'in', 'class', '.', 'Check', 'out', 'the', 'website', ':', 'www.example.com', 'for', 'more', 'information', '!', 'The', 'price', 'is', '$100.50', '.']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hint:\n",
    "\n",
    "Simple Space-Based Tokenization:\n",
    "\n",
    "```\n",
    "import re # Regular Expression library. Will be useful in this exercise\n",
    "\n",
    "def simple_tokenizer(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "sentence = 'This is a simple sentence that can be tokenized using spaces . Note that even punctuation marks are treated as separate tokens .'\n",
    "tokens = simple_tokenizer(sentence)\n",
    "print(tokens)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hint Answer 1:\n",
    "\n",
    "Handling Dates:\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "def handle_dates(tokens):\n",
    "    date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "    modified_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(date_pattern, token):\n",
    "            modified_tokens.append(token)\n",
    "        else:\n",
    "            modified_tokens.extend(re.split(r'(\\W)', token))\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "Rememebre to import re library\n",
    "\n",
    "\n",
    "We used the re.match function to check if the token matches the date pattern. \n",
    "If it does, we append it to the modified_tokens list. \n",
    "If it does not, we use the re.split function to split the token based on the non-word characters and append the resulting tokens to the modified_tokens list.\n",
    "\n",
    "*Also note we are handling a date as pattern like YYYY-MM-DD. Can you think of a way to handle dates in the format MM-DD-YYYY?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "\n",
    "Handling URLs:\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "def handle_urls(tokens):\n",
    "    url_pattern = r'www\\.\\S+|https?://\\S+'\n",
    "    modified_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(url_pattern, token):\n",
    "            modified_tokens.append(token)\n",
    "        else:\n",
    "            modified_tokens.extend(re.split(r'(\\W)', token))\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "\n",
    "Handling Hashtags:\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "def handle_hashtags(tokens):\n",
    "    hashtag_pattern = r'#\\w+'\n",
    "    modified_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(hashtag_pattern, token):\n",
    "            modified_tokens.append(token)\n",
    "        else:\n",
    "            modified_tokens.extend(re.split(r'(\\W)', token))\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "\n",
    "Handling Currency Values:\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "def handle_currency(tokens):\n",
    "    currency_pattern = r'\\$\\d+(\\.\\d+)?'\n",
    "    modified_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(currency_pattern, token):\n",
    "            modified_tokens.append(token)\n",
    "        else:\n",
    "            modified_tokens.extend(re.split(r'(\\W)', token))\n",
    "    return modified_tokens\n",
    "```\n",
    "\n",
    "\n",
    "Combine All Functionalities:\n",
    "\n",
    "```\n",
    "def comprehensive_tokenizer(sentence):\n",
    "    tokens = simple_tokenizer(sentence)\n",
    "    tokens = handle_dates(tokens)\n",
    "    tokens = handle_urls(tokens)\n",
    "    tokens = handle_hashtags(tokens)\n",
    "    tokens = handle_currency(tokens)\n",
    "    return tokens\n",
    "```\n",
    "\n",
    "\n",
    "Applying it on the sentence above:\n",
    "```\n",
    "sentence = \"I bought these shoes for $123.50 from www.shoes.com #shoes #shopping\"\n",
    "result = comprehensive_tokenizer(sentence)\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using nltk\n",
    "\n",
    "NLTK is python library. It is used for natural language processing. It has many features like tokenization, stemming, lemmatization, POS tagging, etc.\n",
    "Today we will see how to use NLTK for tokenization.\n",
    "\n",
    "NLTK offers many different types of tokenizers based on your use case. We will see some of them here.\n",
    "1. Word Tokenizer\n",
    "2. Sentence Tokenizer\n",
    "3. Regexp Tokenizer\n",
    "\n",
    "#### Word Tokenizer\n",
    "* Word tokenizer is used to tokenize the text into words. \n",
    "* It is used to split a sentence into words. It uses the space between two words to split the sentence. \n",
    "* It uses a simple logic that is also quite prone to errors. Let’s see an example of how it works.\n",
    "```\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I am learning NLP\"\n",
    "word_tokenize(text)\n",
    "```\n",
    "\n",
    "\n",
    "#### Sentence Tokenizer\n",
    "* Sentence tokenizer is used to tokenize the text into sentences.\n",
    "* It is used to split a paragraph or a large sentence into sentences.\n",
    "* It uses periods (.) to split the text into sentences.\n",
    "\n",
    "```\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"I am learning NLP. It is very interesting and exciting. I am enjoying this course.\"\n",
    "sent_tokenize(text)\n",
    "```\n",
    "\n",
    "#### Regex Tokenizer\n",
    "* Regex tokenizer is used to tokenize the text using regular expressions.\n",
    "* It is used to split a string into substrings using a regular expression.\n",
    "* It uses regular expressions to split the text into tokens.\n",
    "\n",
    "```\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create a regular expression tokenizer to handle urls\n",
    "tokenizer = RegexpTokenizer(\"www\\.[^\\s]+|https?://[^\\s]+|\\d+\\.?\\d*\")\n",
    "\n",
    "# Tokenize the text\n",
    "text = \"I am learning NLP. It is very interesting and exciting. I am enjoying this course. You can learn more about NLP at https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\"\n",
    "tokenizer.tokenize(text)\n",
    "```\n",
    "We first initialized RegexpTokenizer using a regex pattern. Then we used the tokenize() method to tokenize the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using spacy (a short demo)\n",
    "\n",
    "```\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "```\n",
    "\n",
    "* Note how spacy is able to identify the parts of speech and the dependency relations between the words in the sentence. \n",
    "* It goes beyond tokenization and provides a lot of useful information about the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizing Youtube comments *UNDER PROGRESS*\n",
    "\n",
    "Read the youtube.csv file and tokenize the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "df = pd.read_csv('youtube.csv')\n",
    "\n",
    "# Retain only the CONTENT column\n",
    "df = df[['CONTENT']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk to tokenize the comments\n",
    "import nltk\n",
    "df['TOKENS'] = df['CONTENT'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Complete this section. Demostrate how easy it is to tokenize the comments using nltk. Handle currency, abbreviations, word filtering, urls, etc.\n",
    "- Word normalization: Ask in comments how would you handle don't - don t or do not , U.S.A., what about less obvious abbreviations where periods are unacceptable like L.S.T.M. \n",
    "- Then see how NLTK handles it. Read how it handles it. I can find code snippet in source of the most suitable tokenizer.\n",
    "- See how case handling is done by tokenizer\n",
    "\n",
    "- Move onto english lemmatization. Use example in slides and show how nltk handles it. Maybe write a simple python dictionary that does lemmatization. Show pitfalls. \n",
    "- This stemmer happens to be porter stemmer. A short intro to how rules work with porter stemmer\n",
    "\n",
    "- A very short intro to BPE. Only use the most common BPETokenizer. \n",
    "\n",
    "- Note: All library installs need to go in requirement.txt\n",
    "\n",
    "- Find a good example for German word tokenization. How do your rules stand up against nltk? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex can also act as filters. For example, we can use regex to remove all the abusive words from the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# There are a lot of punctuations and abusive words in the comments. We need to remove them.\n",
    "\n",
    "#List of abusive words\n",
    "abusive_word_list = [''] # KAVERI: Add the abusive words here\n",
    "\n",
    "# Remove the abusive words\n",
    "def remove_abusive_words(tokens):\n",
    "    return [word for word in tokens if word not in abusive_word_list]\n",
    "\n",
    "df['TOKENS'] = df['TOKENS'].apply(remove_abusive_words)\n",
    "\n",
    "\n",
    "# There are a lot of urls in the comments. We need to remove them. \n",
    "# (In some cases we may want to retain them, such as in the case of a web crawler. We may want to extract the urls and crawl them further.)\n",
    "\n",
    "# Remove the urls\n",
    "def remove_urls_and_abusive_words(tokens):\n",
    "    return [word for word in tokens if not word.startswith('http') and word not in abusive_word_list]\n",
    "\n",
    "\n",
    "df['TOKENS'] = df['TOKENS'].apply(remove_urls_and_abusive_words)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
