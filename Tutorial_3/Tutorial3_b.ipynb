{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "def ngramize(filename, n = 2):\n",
    "  \"\"\"\n",
    "    given a file name, generate the ngrams and n-1 grams\n",
    "  \"\"\"\n",
    "  with open(filename, 'r') as f:\n",
    "    lines = f.read()\n",
    "    \n",
    "  sentences = nltk.sent_tokenize(lines)\n",
    "  sentences = [sent.strip().replace(\"\\n\", \" \") \n",
    "                      for sent in sentences]\n",
    "                      \n",
    "  sentences_tok = [nltk.word_tokenize(sent) \n",
    "                      for sent in sentences]\n",
    "                      \n",
    "  sentences_padn = [list(nltk.lm.preprocessing.pad_both_ends(sent, n = n)) \n",
    "                      for sent in sentences_tok]\n",
    "                      \n",
    "  sentences_ngram = [list(nltk.ngrams(sent, n = n)) \n",
    "                      for sent in sentences_padn]\n",
    "  sentences_ngram_minus = [list(nltk.ngrams(sent, n = n-1)) \n",
    "                      for sent in sentences_padn]                      \n",
    "  \n",
    "  flat_ngram = sum(sentences_ngram, [])\n",
    "  flat_ngram_minus = sum(sentences_ngram_minus, [])  \n",
    "                      \n",
    "  return(flat_ngram, flat_ngram_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', '</s>'), 2686),\n",
       " ((',', 'and'), 945),\n",
       " (('<s>', 'I'), 577),\n",
       " (('of', 'the'), 526),\n",
       " ((',', 'I'), 337),\n",
       " ((',', 'but'), 320),\n",
       " (('of', 'my'), 271),\n",
       " (('in', 'the'), 248),\n",
       " (('<s>', 'The'), 243),\n",
       " (('<s>', '“'), 230)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us load the Frankenstein novel and count the unigrams and bigrams\n",
    "\n",
    "bigram, unigram = ngramize(\"./frankenstein.txt\", n = 2)\n",
    "\n",
    "bigram_count = Counter(bigram)\n",
    "unigram_count = Counter(unigram)\n",
    "\n",
    "bigram_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P(He | &lt;s&gt;)',\n",
       " 'P(is | He)',\n",
       " 'P(the | is)',\n",
       " 'P(same | the)',\n",
       " 'P(man | same)',\n",
       " 'P(. | man)',\n",
       " 'P(&lt;/s&gt; | .)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us obtain the bigram strings for the sentence \"He is the same man.\"\n",
    "# Note: We add the beginning of sentence and end of sentence tokens to the sentence - <s> and </s> respectively.\n",
    "\n",
    "def get_conditional_strings(sentence, n = 2):\n",
    "  \"\"\"\n",
    "    given a sentence, return the string of conditionals\n",
    "  \"\"\"\n",
    "  sent_tokens = nltk.word_tokenize(sentence)\n",
    "  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens, n = n)\n",
    "  sent_pad = [x.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\") for x in sent_pad]\n",
    "  sent_ngram = nltk.ngrams(sent_pad, n = n)\n",
    "  out_cond = [f\"P({x[-1]} | {' '.join(x[0:-1])})\" for x in sent_ngram]\n",
    "  return(out_cond)\n",
    "\n",
    "sentence = \"He is the same man.\"\n",
    "get_conditional_strings(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now use the NLTK library to compute the conditional probability of a sentence. We shall see the functions used in cell below.\n",
    "import numpy as np\n",
    "\n",
    "def get_conditional_prob(x, bigram_count, unigram_count):\n",
    "  \"\"\"\n",
    "    for a tuple x, get the conditional probability of x[1] | x[0]\n",
    "  \"\"\"\n",
    "  if x in bigram_count:\n",
    "    cond = bigram_count[x] / unigram_count[x[0:-1]]\n",
    "  else:\n",
    "      cond = 0\n",
    "\n",
    "  return(cond)\n",
    "\n",
    "def get_sentence_probs(sentence, bigram_count, unigram_count, n = 2):\n",
    "  \"\"\"\n",
    "    given a sentence, get its list of conditional probabilities\n",
    "  \"\"\"\n",
    "  sent_tokens = nltk.word_tokenize(sentence)\n",
    "  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens, n = n)\n",
    "  sent_ngram = nltk.ngrams(sent_pad, n = n)\n",
    "  sent_conditionals = [get_conditional_prob(gram, bigram_count, unigram_count) \n",
    "                        for gram in sent_ngram]\n",
    "  return(sent_conditionals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:  ['P(I | &lt;s&gt;)', 'P(saw | I)', 'P(the | saw)', 'P(old | the)', 'P(man | old)', 'P(. | man)', 'P(&lt;/s&gt; | .)'] \n",
      "\n",
      "Conditional probabilities:  [0.18764227642276424, 0.01620288834096513, 0.23404255319148937, 0.006405329233922624, 0.68, 0.13636363636363635, 0.9992559523809523] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I saw the old man.\"\n",
    "cond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\n",
    "cond_surp = [-np.log2(x) for x in cond_probs]\n",
    "cond_strings = get_conditional_strings(sentence, n = 2)\n",
    "\n",
    "print('Bigrams: ', cond_strings, '\\n')\n",
    "print('Conditional probabilities: ', cond_probs, '\\n')\n",
    "\n",
    "# These are the individual conditional probablities for each bigram in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "\n",
    "Perplexity is a measure of how well a probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.\n",
    "Perplexity is defined as the inverse probability of the test set, normalised by the number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "\n",
    "Write a function to compute the perplexity of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_cond_probs_and_strings(sentence, bigram_count, unigram_count, n = 2):\n",
    "    '''Assembles a dataframe with conditional probabilities and strings for a given sentence'''\n",
    "    cond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\n",
    "    cond_surp = [-np.log2(x) for x in cond_probs]\n",
    "    cond_strings = get_conditional_strings(sentence, n = 2)\n",
    "    df = pd.DataFrame({\"Conditional String\": cond_strings, \"Conditional Probability\": cond_probs, \"Conditional Surprisal\": cond_surp})\n",
    "    return df\n",
    "\n",
    "def get_perplexity(df):\n",
    "    \"\"\"calculate perplexity using conditional probabilitiy column\"\"\"\n",
    "    probability_product = 1\n",
    "    for i in range(len(df)):\n",
    "        probability_product = probability_product * df['Conditional Probability'][i]\n",
    "    print(\"Product of probs: \", probability_product)\n",
    "\n",
    "    ###############################################################\n",
    "    ####################Enter your code below######################\n",
    "    perplexity = probability_product ** (-1/len(df))\n",
    "    ###############################################################\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conditional String</th>\n",
       "      <th>Conditional Probability</th>\n",
       "      <th>Conditional Surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P(I | &amp;lt;s&amp;gt;)</td>\n",
       "      <td>0.187642</td>\n",
       "      <td>2.413943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P(saw | I)</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>5.947605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P(the | saw)</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>2.095157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P(old | the)</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>7.286512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P(man | old)</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.556393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Conditional String  Conditional Probability  Conditional Surprisal\n",
       "0   P(I | &lt;s&gt;)                 0.187642               2.413943\n",
       "1         P(saw | I)                 0.016203               5.947605\n",
       "2       P(the | saw)                 0.234043               2.095157\n",
       "3       P(old | the)                 0.006405               7.286512\n",
       "4       P(man | old)                 0.680000               0.556393"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us now compute the conditional probability of a sentence\n",
    "# We will use the bigram_count and unigram_count dictionaries to compute unigram and bigram probabilities\n",
    "# These probabilities will be used to compute the conditional probability of a sentence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sentence = \"I saw the old man.\"\n",
    "cond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\n",
    "cond_surp = [-np.log2(x) for x in cond_probs]\n",
    "cond_strings = get_conditional_strings(sentence, n = 2)\n",
    "\n",
    "# Let us now create a dataframe to display/store the conditional probabilities and surprisals\n",
    "df = pd.DataFrame({\"Conditional String\": cond_strings, \"Conditional Probability\": cond_probs, \"Conditional Surprisal\": cond_surp})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of probs:  4.223219432797573e-07\n",
      "Perplexity:  8.139961224506271\n"
     ]
    }
   ],
   "source": [
    "# After completing question 1, we can also compute the perplexity of the model using the conditional probabilities of the sentence stored in the dataframe.\n",
    "\n",
    "perplexity = get_perplexity(df)\n",
    "print(\"Perplexity: \", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A familiar problem approaches\n",
    "\n",
    "But, not everything is so neat and tidy. Let’s try this again for the sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13802/3720152318.py:8: RuntimeWarning: divide by zero encountered in log2\n",
      "  cond_surp = [-np.log2(x) for x in cond_probs]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conditional String</th>\n",
       "      <th>Conditional Probability</th>\n",
       "      <th>Conditional Surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P(I | &amp;lt;s&amp;gt;)</td>\n",
       "      <td>0.187642</td>\n",
       "      <td>2.413943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P(saw | I)</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>5.947605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P(the | saw)</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>2.095157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P(same | the)</td>\n",
       "      <td>0.015373</td>\n",
       "      <td>6.023477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P(man | same)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Conditional String  Conditional Probability  Conditional Surprisal\n",
       "0   P(I | &lt;s&gt;)                 0.187642               2.413943\n",
       "1         P(saw | I)                 0.016203               5.947605\n",
       "2       P(the | saw)                 0.234043               2.095157\n",
       "3      P(same | the)                 0.015373               6.023477\n",
       "4      P(man | same)                 0.000000                    inf"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I saw the same man.\"\n",
    "\n",
    "df = get_cond_probs_and_strings(sentence, bigram_count, unigram_count, n = 2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of probs:  0.0\n",
      "Perplexity:  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13802/3720152318.py:22: RuntimeWarning: divide by zero encountered in scalar power\n",
      "  perplexity = probability_product ** (-1/len(df))\n"
     ]
    }
   ],
   "source": [
    "perplexity = get_perplexity(df)\n",
    "print(\"Perplexity: \", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV (Out-of-vocabulary) words / Data sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the bigram (\"same\", \"man\") just didn’t appear in the novel. This is zero percolates up through all of our calculations.\n",
    "\n",
    "<img src=\"./OOV Surprisal.png\" alt=\"Alternative text\" height=300 rem/>\n",
    "\n",
    "Since our model has not seen the bigram occur in the corpus, the conditional probability is computed as zero. \n",
    "Of course this is not true as the bigram is a fairly common term in the English language.\n",
    "\n",
    "This is, of course <font color=\"cyan\"> data sparsity </font> rearing its head again. On the one hand, we are building an n-gram model out of a fairly small corpus. But on the other, the data sparsity problem will never go away, and we are always going to be left with the following two issues:\n",
    "\n",
    "* Out Of Vocabulary items\n",
    "* Missing ngrams of words that were in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example of perplexity blowing up was due to a specific bigram, ('same', 'man') not appearing in the corpus, even though each individual word does appear. The same thing will happen if any individual word in a sentence is oov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing\n",
    "\n",
    "We can address this by smoothing the conditional probability estimates. One of the simplest smoothing methods is called Laplace smoothing, which adds one to the numerator and V to the denominator:\n",
    "\n",
    "<img src=\"laplace smoothing.png\" height=100rem/>\n",
    "\n",
    "where V is the number of words in the vocabulary. This is also called add-one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "\n",
    "Compute the smoothed bigram probabilities and complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer2:\n",
    "\n",
    "def get_conditional_prob_smooth(x, bigram_count, unigram_count):\n",
    "    \"\"\"\n",
    "    for a tuple x, get the conditional probability of x[1] | x[0]\n",
    "    Perform smoothing\n",
    "    \"\"\"\n",
    "\n",
    "    #########################################################################\n",
    "    ##############################Enter your code here#######################\n",
    "    if x in bigram_count:\n",
    "        cond = (bigram_count[x]+1) / (unigram_count[x[0:-1]] + len(unigram_count))\n",
    "    else:\n",
    "        cond = 1/ (unigram_count[x[0:-1]] + len(unigram_count))\n",
    "\n",
    "    #########################################################################\n",
    "    return(cond)\n",
    "\n",
    "def get_sentence_probs_smooth(sentence, bigram_count, unigram_count, n = 2):\n",
    "  \"\"\"\n",
    "    given a sentence, get its list of conditional probabilities\n",
    "  \"\"\"\n",
    "  sent_tokens = nltk.word_tokenize(sentence)\n",
    "  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens, n = n)\n",
    "  sent_ngram = nltk.ngrams(sent_pad, n = n)\n",
    "  sent_conditionals = [get_conditional_prob_smooth(gram, bigram_count, unigram_count) \n",
    "                        for gram in sent_ngram]\n",
    "  return(sent_conditionals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_probs_and_strings_smooth(sentence, bigram_count, unigram_count, n = 2):\n",
    "    cond_probs = get_sentence_probs_smooth(sentence, bigram_count, unigram_count, n = 2)\n",
    "    cond_surp = [-np.log2(x) for x in cond_probs]\n",
    "    cond_strings = get_conditional_strings(sentence, n = 2)\n",
    "    df = pd.DataFrame({\"Conditional String\": cond_strings, \"Conditional Probability\": cond_probs, \"Conditional Surprisal\": cond_surp})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of probs:  2.97335737558768e-16\n",
      "Perplexity:  165.23709667263265\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conditional String</th>\n",
       "      <th>Conditional Probability</th>\n",
       "      <th>Conditional Surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P(I | &amp;lt;s&amp;gt;)</td>\n",
       "      <td>0.053583</td>\n",
       "      <td>4.222080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P(saw | I)</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>7.810503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P(the | saw)</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>8.406806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P(same | the)</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>7.572964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P(man | same)</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>12.924441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Conditional String  Conditional Probability  Conditional Surprisal\n",
       "0   P(I | &lt;s&gt;)                 0.053583               4.222080\n",
       "1         P(saw | I)                 0.004455               7.810503\n",
       "2       P(the | saw)                 0.002946               8.406806\n",
       "3      P(same | the)                 0.005252               7.572964\n",
       "4      P(man | same)                 0.000129              12.924441"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_smooth = get_cond_probs_and_strings_smooth(sentence, bigram_count, unigram_count, n = 2)\n",
    "\n",
    "perplexity = get_perplexity(df_smooth)\n",
    "print(\"Perplexity: \", perplexity)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "df_smooth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add k Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By smoothing out the probabilities unseen words/OOV words will not have a probability of 0.\n",
    "\n",
    "**Add 1 smoothing** adds 1 to the numerator and the number of words in the vocabulary to the denominator.\n",
    "\n",
    "\n",
    "**Add k smoothing** adds k to the numerator and k times the number of words in the vocabulary to the denominator.\n",
    "\n",
    "k can be chosen to be 1, 0.5, 0.1, 0.01, 0.001, etc.\n",
    "This depends on the number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* https://jofrhwld.github.io/teaching/courses/2022_lin517/lectures/ngram/01-ngram-eval.html\n",
    "* https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_ling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
